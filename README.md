# Sistema de Informes Periciales

![Version](https://img.shields.io/badge/version-2.0.0-blue)
![Python](https://img.shields.io/badge/python-3.10+-green)
![Flask](https://img.shields.io/badge/flask-3.0.0-red)
![IA](https://img.shields.io/badge/IA-Llama%203.1-purple)

Sistema web para la redacciÃ³n de informes periciales motivados mediante el **MÃ©todo Formal Causal**, potenciado por IA local (LangGraph + Ollama).

## ğŸ¯ CaracterÃ­sticas

- âœ… **GeneraciÃ³n automÃ¡tica con IA** - AnÃ¡lisis contextualizado mediante Llama 3.1
- âœ… **Procesamiento 100% local** - Sin envÃ­o de datos externos (privacidad total)
- âœ… **MÃ©todo Formal Causal** - Estructura de 3 preguntas: Â¿Por quÃ©?, Â¿Para quÃ©?, Â¿QuÃ© es?
- âœ… **Informes profesionales** - 11 secciones listas para uso legal
- âœ… **Mapa conceptual interactivo** - VisualizaciÃ³n navegable del anÃ¡lisis
- âœ… **Interfaz moderna** - DiseÃ±o responsive con animaciones

---

## ğŸš€ InstalaciÃ³n RÃ¡pida

### 1. Requisitos Previos

- **Python 3.10 o superior**
- **Ollama** para ejecutar modelos IA localmente
- **uv** (gestor de paquetes ultrarrÃ¡pido)

### 2. Instalar Ollama

#### Windows
```powershell
# Descargar e instalar desde: https://ollama.ai/download
# O usar winget:
winget install Ollama.Ollama
```

#### Verificar instalaciÃ³n
```powershell
ollama --version
```

#### Descargar modelo Llama 3.1
```powershell
ollama pull llama3.1
```

Esto descargarÃ¡ ~4.7 GB. El modelo se ejecuta completamente en tu mÃ¡quina.

### 3. Instalar uv

#### Â¿Por quÃ© uv en lugar de pip?

| CaracterÃ­stica | uv | pip |
|----------------|-----|-----|
| **Velocidad** | âš¡ 10-100x mÃ¡s rÃ¡pido | ğŸ¢ Lento |
| **GestiÃ³n de venv** | ğŸ¤– AutomÃ¡tica | ğŸ”§ Manual |
| **Lock file** | âœ… uv.lock garantiza reproducibilidad | âŒ Sin lock file nativo |
| **ResoluciÃ³n de dependencias** | ğŸš€ UltrarrÃ¡pida | ğŸ• Lenta en proyectos grandes |
| **InstalaciÃ³n** | ğŸ“¦ Todo en uno (pip + pip-tools + venv) | ğŸ”€ MÃºltiples herramientas |
| **Escritura en Rust** | âœ… Optimizado y seguro | âš ï¸ Python (mÃ¡s lento) |

**Comparativa real**:
```
Instalar 40 paquetes:
pip: ~45 segundos
uv:  ~2 segundos  âš¡
```

#### Instalar uv en Windows
```powershell
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

#### Verificar instalaciÃ³n
```powershell
uv --version
```

### 4. Clonar e Instalar el Proyecto

```powershell
# Clonar repositorio
git clone https://github.com/jpb75/Informes-Periciales.git
cd Informes-Periciales

# Sincronizar dependencias (crea .venv automÃ¡ticamente)
uv sync
```

**Â¿QuÃ© hace `uv sync`?**
1. Lee `pyproject.toml`
2. Crea entorno virtual en `.venv/`
3. Instala todas las dependencias
4. Genera `uv.lock` para reproducibilidad

---

## ğŸ’» Uso

### Ejecutar la AplicaciÃ³n

```powershell
uv run python app.py
```

### Abrir en el Navegador

```
http://localhost:5000
```

### Flujo de Uso

1. **Introduce tu conjetura** en el formulario (describe el caso pericial)
2. **Espera 30-60 segundos** mientras la IA procesa
3. **Visualiza tu informe** en formato profesional o mapa conceptual
4. **Imprime o exporta** el resultado

---

## ğŸ“Š El MÃ©todo Formal Causal

MetodologÃ­a estructurada para anÃ¡lisis periciales basada en 3 preguntas fundamentales:

### 1. Â¿Por quÃ©? - Motivaciones (4 tipos)

- **Preceptivas**: Del enunciado del problema
- **TÃ©cnicas**: Leyes, normas y regulaciones aplicables
- **Facultativas**: MotivaciÃ³n profesional del perito
- **Progresistas**: AportaciÃ³n al conocimiento actual

### 2. Â¿Para quÃ©? - Objetivos

6-8 objetivos relacionados con cada tipo de motivaciÃ³n.

### 3. Â¿QuÃ© es? - DefiniciÃ³n

DefiniciÃ³n precisa y contextualizaciÃ³n del problema.

---

## ğŸ¤– CÃ³mo Funciona la IA

```
Conjetura del usuario
      â†“
Agente LangGraph (6 pasos secuenciales)
      â”œâ”€ 1. Analiza motivaciones PRECEPTIVAS
      â”œâ”€ 2. Analiza motivaciones TÃ‰CNICAS
      â”œâ”€ 3. Analiza motivaciones FACULTATIVAS
      â”œâ”€ 4. Analiza motivaciones PROGRESISTAS
      â”œâ”€ 5. Genera OBJETIVOS vinculados
      â””â”€ 6. Define QUÃ‰ ES el problema
      â†“
Informe completo (11 secciones)
```

**Cada paso** usa el contexto de los pasos anteriores para mantener coherencia y relaciÃ³n entre todos los elementos del anÃ¡lisis.

---

## ğŸ“ Estructura del Proyecto

```
Informes-Periciales/
â”œâ”€â”€ app.py                          # Backend Flask
â”œâ”€â”€ pyproject.toml                  # Config + dependencias
â”œâ”€â”€ uv.lock                         # Lock file
â”‚
â”œâ”€â”€ agentes/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ formal_causal_agent.py     # Agente IA (LangGraph + prompts)
â”‚
â”œâ”€â”€ templates/                      # HTML
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ informe.html
â”‚   â””â”€â”€ mapa_conceptual_v2.html
â”‚
â”œâ”€â”€ static/                         # CSS + JavaScript
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ GUIA_COMPLETA.md           # DocumentaciÃ³n tÃ©cnica detallada
â”‚
â””â”€â”€ README.md                       # Este archivo
```

---

## ğŸ› ï¸ TecnologÃ­as

### Backend
- **Flask 3.0** - Framework web
- **Python 3.10+** - Lenguaje base

### IA
- **LangGraph** - Framework de agentes con grafos de estado
- **LangChain** - AbstracciÃ³n para LLMs
- **Ollama** - Runtime local de modelos IA
- **Llama 3.1** - Modelo de lenguaje de Meta (4.7 GB)

### GestiÃ³n
- **uv** - Gestor de paquetes ultrarrÃ¡pido
- **pyproject.toml** - ConfiguraciÃ³n estÃ¡ndar Python

---

## âš™ï¸ ConfiguraciÃ³n

### Cambiar el Modelo de IA

Edita `agentes/formal_causal_agent.py`:

```python
OLLAMA_MODEL = "llama3.1"  # Cambiar aquÃ­
OLLAMA_TEMPERATURE = 0.3   # Ajustar creatividad (0.1-1.0)
```

Modelos disponibles: https://ollama.ai/library

### Modificar Prompts

Los prompts estÃ¡n en `agentes/formal_causal_agent.py`. Edita las constantes `PROMPT_PRECEPTIVAS`, `PROMPT_TECNICAS`, etc.

---

## ğŸ”§ Comandos Ãštiles con uv

```powershell
# Instalar dependencias
uv sync

# Ejecutar aplicaciÃ³n
uv run python app.py

# AÃ±adir nueva dependencia
uv add nombre-paquete

# AÃ±adir dependencia de desarrollo
uv add --dev nombre-paquete

# Actualizar dependencias
uv sync --upgrade

# Ver dependencias instaladas
uv pip list
```

---

## â“ SoluciÃ³n de Problemas

### Ollama no responde
```powershell
# Verificar que estÃ¡ corriendo
ollama serve
```

### Modelo no encontrado
```powershell
# Descargar el modelo
ollama pull llama3.1
```

### Error al importar mÃ³dulos
```powershell
# Resincronizar dependencias
uv sync
```

### Procesamiento muy lento

**Causas**:
- Primera ejecuciÃ³n (carga modelo en memoria)
- RAM insuficiente (< 8GB)

**Soluciones**:
- Esperar a segunda ejecuciÃ³n (serÃ¡ mÃ¡s rÃ¡pida)
- Usar modelo mÃ¡s pequeÃ±o: `ollama pull llama3.2:1b`

---

## ğŸ“š DocumentaciÃ³n Completa

Para informaciÃ³n tÃ©cnica detallada, arquitectura del sistema, y guÃ­as avanzadas:

ğŸ‘‰ **[docs/GUIA_COMPLETA.md](docs/GUIA_COMPLETA.md)**

---

## ğŸ¯ Ejemplo de Uso

### Conjetura de Ejemplo

```
Se requiere evaluar si un edificio de viviendas de 5 plantas cumple 
con la normativa vigente de eficiencia energÃ©tica y accesibilidad. 
Los vecinos reportan problemas de humedad en las plantas bajas y 
ausencia de rampa de acceso para personas con movilidad reducida.
```

### Resultado

El sistema generarÃ¡ automÃ¡ticamente:
- 4 tipos de motivaciones (10-14 en total)
- 6-8 objetivos contextualizados
- DefiniciÃ³n precisa del problema
- Informe de 11 secciones profesionales

**Tiempo de procesamiento**: 30-60 segundos

---

## ğŸ“ Requisitos del Sistema

- **Python**: 3.10 o superior
- **RAM**: 8 GB mÃ­nimo, 16 GB recomendado
- **Espacio**: 5 GB para modelo Llama 3.1
- **CPU**: Procesador multinÃºcleo moderno
- **GPU**: Opcional (acelera procesamiento)
- **SO**: Windows, macOS, Linux

---

## ğŸ¤ Contribuciones

Las sugerencias y mejoras son bienvenidas. Este es un proyecto en desarrollo activo.

---

## ğŸ“„ Licencia

Proyecto desarrollado para uso acadÃ©mico y profesional en el Ã¡mbito de informes periciales.

---

## ğŸ‘¤ Autor

Sistema desarrollado para la investigaciÃ³n y redacciÃ³n de informes periciales motivados.

**Repositorio**: https://github.com/jpb75/Informes-Periciales  
**VersiÃ³n**: 2.0.0  
**Ãšltima actualizaciÃ³n**: Noviembre 2025

---

## ğŸ”— Enlaces Ãštiles

- [Ollama](https://ollama.ai) - Runtime de modelos IA
- [uv](https://docs.astral.sh/uv/) - Gestor de paquetes Python
- [LangGraph](https://langchain-ai.github.io/langgraph/) - Framework de agentes
- [Flask](https://flask.palletsprojects.com/) - Framework web Python
